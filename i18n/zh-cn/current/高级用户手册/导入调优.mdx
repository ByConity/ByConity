---
title: 导入调优
tags:
  - Docs
---

# 导入调优

# 支持的导入方式

目前 CNCH 支持的导入方式有如下几种：

- INSERT INFILE
- 适合已经生成好数据文件的情况，在数据量较少的情况可以直接读取远程/本地的数据文件导入 CNCH，数据量大的情况(超过数 GB)下建议走 PartWriter 和 Attach 的方式
- INSERT VALUES
- 适合临时插入少量的数据用来测试
- INSERT SELECT
- 适合需要保存某张表结果并供后续查询的情况
- Dumper&Attach
- 适合从本地版迁移到 CNCH 的情况，可以直接将整个集群的数据直接用 Dumper 写入 HDFS 中，再利用 Attach 将这些数据直接移动到 CNCH 中并进行查询
- PartWriter&Attach
- 对于数据量大且文件数量多的场景，可以利用 Spark 等任务用 PartWriter 将原始的文件生成新的 Part 文件，并写入 HDFS 中，随后可以利用 Attach 功能将数据移动到 CNCH 中并进行查询

# 调优手段

## 直接写入方式调优

在使用 INSERT VALUES, INSERT INFILE 或者 PartWriter 工具写入时，最后生成的 Part 数量会影响写入 HDFS 的次数进而影响写入整体的耗时，因此应当尽量减少 Part 的数量。直接写入的流程如下：

- 读取部分文件数据
- 将这部分数据按照 PartitionBy 进行切分
- 将这部分数据按照 ClusterBy 进行切分
- 将切分完的数据写成新的 Part 并写入 HDFS

调优手段：

1. 为了减少 Part 的数量，我们可以将文件中具有相同的分区和 Bucket 的数据排列在一起，这样每次读取一些新的数据后，生成的 Part 数量会尽可能少。可以将数据按照分区相同，分区内 Bucket 相同的要求进行排序，Bucket 的计算规则是：

- 如果没有指定 SPLIT_NUMBER，会将 ClusterByKey 所使用的列计算 SipHash 后对 BucketNumber 取模得到 BucketNumber
- 如果指定了 SPLIT_NUMBER
- 计算 SplitValue
- 如果 ClusterBy 某一列，利用 dtspartition 函数计算出对应的 SplitValue
- 如果 ClusterBy 多列，则将这些列利用 SipHash 计算出对应的 SplitValue
- 计算 BucketNumber
- 如果是 WithRange，则用 SplitValue \* BucketCount / SplitNumber 计算对应 BucketNumber
- 如果不是 WithRange，则用 SplitValue % BucketCount 计算对应 BucketNumber

1. 读取文件时
2. 如果每行数据大小并不大，可以通过调大 max_insert_block_size 来一次读取更大的 Block，从而生成更大的 Part
3. 如果读取的文件不是 HDFS/CFS 的，同时使用通配符匹配了多个文件，也可以同时调大 min_insert_block_size_rows 和 min_insert_block_size_bytes

## Dumper 方式调优

Dumper 迁移的时候可以一次指定一张表及其中的部分分区，将其写到 HDFS 中，这部分耗时与表的数据大小以及 Part 数量有关，可以使用 parallel 参数调大上传 Part 的并发，同时也可以等待 Part 在本地先 Merge 的比较充分后再使用 Dumper 进行上传。
